== Kafka Batch Consumer with Manual commit

In this sample you'll use the Kafka Batch Source Kamelet in action and write the single records of the batch into an S3 bucket.

=== Install JBang

First install JBang according to https://www.jbang.dev

When JBang is installed then you should be able to run from a shell:

[source,sh]
----
$ jbang --version
----

This will output the version of JBang.

To run this example you can either install Camel on JBang via:

[source,sh]
----
$ jbang app install camel@apache/camel
----

Which allows to run CamelJBang with `camel` as shown below.

=== Setup Kafka instance

You'll need to run a Kafka cluster to point to. In this case you could use an ansible role like https://github.com/oscerd/kafka-ansible-role

And set up a file deploy.yaml with the following content:

```yaml
- name: role kafka
  hosts: localhost
  remote_user: user
  
  roles:
    - role: kafka-ansible-role
      kafka_version: 3.4.1
      path_dir: /home/user/
      unarchive_dest_dir: /home/user/kafka/demo/
      start_kafka: true
```

and then run

```shell script
ansible-playbook -v deploy.yaml
```

This should start a Kafka instance for you, on your local machine.

=== Set up AWS S3

Create a bucket on your personal account.

The Kamelet will use the defaultCredentialsProvider, so you'll need the credentials file on your hosts.

Modify the kafka-batch-s3.yaml file to add the correct region and the correct bucket name.

=== How to run

Then you can run this example using:

[source,sh]
----
$ jbang -Dcamel.jbang.version=4.4.0-SNAPSHOT camel@apache/camel run --local-kamelet-dir=<path_to_kamelets_repository> kafka-batch-s3.camel.yaml
----

=== Consumer running

You should see:

[source,sh]
----
2024-02-16 10:19:47.357  INFO 17500 --- [           main] el.impl.engine.AbstractCamelContext : Routes startup (started:4)
2024-02-16 10:19:47.357  INFO 17500 --- [           main] el.impl.engine.AbstractCamelContext :     Started kafka-to-log (kamelet://kafka-batch-not-secured-source)
2024-02-16 10:19:47.357  INFO 17500 --- [           main] el.impl.engine.AbstractCamelContext :     Started kafka-batch-not-secured-source-1 (kafka://test-topic)
2024-02-16 10:19:47.358  INFO 17500 --- [           main] el.impl.engine.AbstractCamelContext :     Started aws-s3-sink-2 (kamelet://source)
2024-02-16 10:19:47.358  INFO 17500 --- [           main] el.impl.engine.AbstractCamelContext :     Started kafka-batch-manual-commit-action-3 (kamelet://source)
2024-02-16 10:19:47.358  INFO 17500 --- [           main] el.impl.engine.AbstractCamelContext : Apache Camel 4.4.0-SNAPSHOT (kafka-batch-s3) started in 1s244ms (build:0ms init:0ms start:1s244ms)
2024-02-16 10:19:47.418  INFO 17500 --- [mer[test-topic]] he.kafka.common.utils.AppInfoParser : Kafka version: 3.6.1
2024-02-16 10:19:47.418  INFO 17500 --- [mer[test-topic]] he.kafka.common.utils.AppInfoParser : Kafka commitId: 5e3c2b738d253ff5
2024-02-16 10:19:47.419  INFO 17500 --- [mer[test-topic]] he.kafka.common.utils.AppInfoParser : Kafka startTimeMs: 1708075187417
2024-02-16 10:19:47.424  INFO 17500 --- [mer[test-topic]] ort.classic.AssignmentAdapterHelper : Using NO-OP resume strategy
2024-02-16 10:19:47.424  INFO 17500 --- [mer[test-topic]] l.component.kafka.KafkaFetchRecords : Subscribing test-topic-Thread 0 to topic test-topic
2024-02-16 10:19:47.425  INFO 17500 --- [mer[test-topic]] afka.clients.consumer.KafkaConsumer : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2024-02-16 10:19:47.693  INFO 17500 --- [mer[test-topic]] org.apache.kafka.clients.Metadata   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: QKy-eUclRryoTxWZq4xsPA
2024-02-16 10:19:47.694  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator ghost:9092 (id: 2147483647 rack: null)
2024-02-16 10:19:47.697  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2024-02-16 10:19:47.710  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-083abfe1-f3d1-4f52-ad0a-c8118c711733
2024-02-16 10:19:47.712  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
2024-02-16 10:19:47.712  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2024-02-16 10:19:47.718  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=19, memberId='consumer-my-group-1-083abfe1-f3d1-4f52-ad0a-c8118c711733', protocol='range'}
2024-02-16 10:19:47.725  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 19: {consumer-my-group-1-083abfe1-f3d1-4f52-ad0a-c8118c711733=Assignment(partitions=[test-topic-0])}
2024-02-16 10:19:47.733  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=19, memberId='consumer-my-group-1-083abfe1-f3d1-4f52-ad0a-c8118c711733', protocol='range'}
2024-02-16 10:19:47.734  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2024-02-16 10:19:47.737  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2024-02-16 10:19:47.749  INFO 17500 --- [mer[test-topic]] sumer.internals.ConsumerCoordinator : [Consumer clientId=consumer-my-group-1, groupId=my-group] Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[ghost:9092 (id: 0 rack: null)], epoch=0}}

----

At this point we should start sending messages to the test-topic topic. We could use kcat for this.

[source,sh]
----
for i in {1..2}; do echo "hello there" | kcat -b localhost:9092 -P -t test-topic; done
----

If you check the situation for the consumer group 'my-group' you could see that the commit happened manually by using the kafka-batch-manual-commit-action.

[source,sh]
----
./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
my-group        test-topic      0          2               2               0               -               -               -
----

You could also try to send groups of 10 records to see how the batch consumer behaves:

[source,sh]
----
for i in {1..50}; do echo "hello there" | kcat -b localhost:9092 -P -t test-topic; done
----

When the process complete you can check your aws bucket:

[source,sh]
----
$ aws s3 ls s3://<bucket_name>
2024-02-16 10:20:57         10 test-topic-20240216102055784.txt
2024-02-16 10:20:57         10 test-topic-20240216102056153.txt
2024-02-16 10:20:57         10 test-topic-20240216102056281.txt
2024-02-16 10:20:57         10 test-topic-20240216102056409.txt
2024-02-16 10:20:57         10 test-topic-20240216102056541.txt
2024-02-16 10:20:57         10 test-topic-20240216102056667.txt
2024-02-16 10:20:57         10 test-topic-20240216102056803.txt
2024-02-16 10:20:57         10 test-topic-20240216102056930.txt
.
.
.
.
----

And you could also verify the content

[source,sh]
----
$ aws s3 cp s3://<bucket_name>/test-topic-20240216102055784.txt -
hello there
----

If you check again the offset for the consumers of my-group group you'll notice we are at offset 52 now.

[source,sh]
----
./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                              HOST            CLIENT-ID
my-group        test-topic      0          52              52              0               -                                                        -               -
----

=== Help and contributions

If you hit any problem using Camel or have some feedback, then please
https://camel.apache.org/community/support/[let us know].

We also love contributors, so
https://camel.apache.org/community/contributing/[get involved] :-)

The Camel riders!
